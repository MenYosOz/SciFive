{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mednli_1",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPd3O1Xl7CA"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Ar53sBl7CB"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDVvzNA7l7CB"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1CKuTdVl7CE"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPI1TmAeh0_"
      },
      "source": [
        "import gin\n",
        "import subprocess\n",
        "gin.parse_config_file(\n",
        "        'gs://t5_training/t5-data/config/pretrained_models_google_base_operative_config.gin'\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtIvOasl7CI"
      },
      "source": [
        "## Register Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JgSsuel7EC"
      },
      "source": [
        "### Mednli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJt-8jsUl7EC"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/mednli/train.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/mednli/test.tsv'\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"mednli: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhK5bXycl7EE"
      },
      "source": [
        "t5.data.TaskRegistry.remove('mednli')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"mednli\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceK0-uXzl7EO"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"all_bioT5\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all_bioT5\",\n",
        "    ['mednli'],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwvZLQy4wv0I"
      },
      "source": [
        "# !gsutil -m rm -r {MODEL_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSzYqw_l7EP"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v1\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pubmed_v2\"\n",
        "BASE_PRETRAINED_DIR = \"gs://t5_training/models/export_models/bio/pmc_v4_1200k\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "# MODEL_DIR = \"gs://t5_training/models/bio/re_v2\"\n",
        "MODEL_DIR = \"gs://t5_training/models/bio/mednli_pmc_v4\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128*2, 8),\n",
        "    \"large\": (8, 64*2, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 15},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkygCUDl7EQ"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"mednli\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW"
      },
      "source": [
        "tasks = [\n",
        "         ['mednli', 'mednli'],\n",
        "         ]\n",
        "output_dir = \"mednli_pmc_v4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAzDXwIlI16q"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "# question_1 = \"Emerin is a nuclear membrane protein which is missing or defective in Emery-Dreifuss muscular dystrophy (EDMD). It is one member of a family of lamina-associated proteins which includes LAP1, LAP2 and lamin B receptor (LBR). A panel of 16 monoclonal antibodies (mAbs) has been mapped to six specific sites throughout the emerin molecule using phage-displayed peptide libraries and has been used to localize emerin in human and rabbit heart. Several mAbs against different emerin epitopes did not recognize intercalated discs in the heart, though they recognized cardiomyocyte nuclei strongly, both at the rim and in intranuclear spots or channels. A polyclonal rabbit antiserum against emerin did recognize both nuclear membrane and intercalated discs but, after affinity purification against a pure-emerin band on a western blot, it stained only the nuclear membrane. These results would not be expected if immunostaining at intercalated discs were due to a product of the emerin gene and, therefore, cast some doubt upon the hypothesis that cardiac defects in EDMD are caused by absence of emerin from intercalated discs. Although emerin was abundant in the membranes of cardiomyocyte nuclei, it was absent from many non-myocyte cells in the heart. This distribution of emerin was similar to that of lamin A, a candidate gene for an autosomal form of EDMD. In contrast, lamin B1 was absent from cardiomyocyte nuclei, showing that lamin B1 is not essential for localization of emerin to the nuclear lamina. Lamin B1 is also almost completely absent from skeletal muscle nuclei. In EDMD, the additional absence of lamin B1 from heart and skeletal muscle nuclei which already lack emerin may offer an alternative explanation of why these tissues are particularly affected..\" \n",
        "# question_2 = \"Molecular analysis of the APC gene in 205 families: extended genotype-phenotype correlations in FAP and evidence for the role of APC amino acid changes in colorectal cancer predisposition.\" \n",
        "# question_3 = \"Who are the 4 members of The Beatles?\" \n",
        "# question_4 = \"How many teeth do humans have?\"\n",
        "\n",
        "# questions = [question_2]\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  task = t[1]\n",
        "  input_file = task + '_predict_input.txt'\n",
        "  output_file = task + '_predict_output.txt'\n",
        "\n",
        "\n",
        "  # Write out the supplied questions to text files.\n",
        "  predict_inputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, input_file)\n",
        "  predict_outputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, output_dir , MODEL_SIZE, output_file)\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        # Select the most probable output token at each step.\n",
        "        # vocabulary=t5.data.SentencePieceVocabulary(vocab),\n",
        "\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + task)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['mednli', 'mednli'],\n",
        "         ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufW-7d5DHE0q"
      },
      "source": [
        "for task in tasks:\n",
        "  # t5_training/t5-data/bio_data/euadr/predicted_output\n",
        "  !gsutil -m cp gs://scifive/finetune/{task[0]}/{output_dir}/{MODEL_SIZE}/{task[1]}_predict_output.txt-* . \n",
        "  !gsutil cp gs://scifive/finetune/{task[0]}/{task[1]}_actual_output.txt . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UA9r05mG7Mu"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, recall_score, precision_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7odWGzG7p6"
      },
      "source": [
        "def convert_RE_labels(filename):\n",
        "    labels = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            labels.append(line.strip().upper())\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuvjLJZDHDNQ"
      },
      "source": [
        "checkpoint = 1245000\n",
        "total_f1 = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "anchor_pred_labels = []\n",
        "anchor_actual_labels = []\n",
        "for task in tasks:\n",
        "    d = task[0]\n",
        "    t = task[1]\n",
        "    \n",
        "    pred_file = os.path.join('/content/', t +'_predict_output.txt-%s'%checkpoint)\n",
        "    actual_file = os.path.join('/content/', t + '_actual_output.txt')\n",
        "    \n",
        "\n",
        "    pred_labels = convert_RE_labels(pred_file)\n",
        "    actual_labels = convert_RE_labels(actual_file)\n",
        "    print(\"Report %s:\"%t, classification_report(actual_labels, pred_labels, digits=4))\n",
        "    f1_score(y_pred=pred_labels, y_true=actual_labels, average='micro')\n",
        "    p,r,f,_ = precision_recall_fscore_support(y_pred=pred_labels, y_true=actual_labels)\n",
        "    results = dict()\n",
        "    results[\"f1 score\"] = f[1]\n",
        "    results[\"recall\"] = r[1]\n",
        "    results[\"precision\"] = p[1]\n",
        "    results[\"specificity\"] = r[0]     \n",
        "    print(t, results) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "chemprot_1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxPd3O1Xl7CA"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1Ar53sBl7CB"
      },
      "source": [
        "## Set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDVvzNA7l7CB"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1CKuTdVl7CE"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaPI1TmAeh0_"
      },
      "source": [
        "import gin\n",
        "import subprocess\n",
        "gin.parse_config_file(\n",
        "        'gs://t5_training/t5-data/config/pretrained_models_google_base_operative_config.gin'\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xtIvOasl7CI"
      },
      "source": [
        "## Register RE Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-JgSsuel7EC"
      },
      "source": [
        "### Chemprot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJt-8jsUl7EC"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://gs://scifive/finetune/chemprot/train.txt_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs:/gs://scifive/finetune/chemprot/test.txt_cleaned.tsv'\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"chemprot_re: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhK5bXycl7EE"
      },
      "source": [
        "t5.data.TaskRegistry.remove('chemprot_re')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"chemprot_re\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAqbaQ7Ol7EK"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceK0-uXzl7EO"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"re_all\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"re_all\",\n",
        "    ['chemprot_re'],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y7-5c2Gl7EO"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwvZLQy4wv0I"
      },
      "source": [
        "# !gsutil -m rm -r {MODEL_DIR}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmSzYqw_l7EP"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"large\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v1\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pubmed_v2\"\n",
        "BASE_PRETRAINED_DIR = \"gs://t5_training/models/export_models/bio/pmc_v4_1100k\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "# MODEL_DIR = \"gs://t5_training/models/bio/re_v2\"\n",
        "MODEL_DIR = \"gs://t5_training/models/bio/chemprot_pmc_v4\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128*2, 8),\n",
        "    \"large\": (8, 64*2, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 15},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-vSwM9Tl7EQ"
      },
      "source": [
        "## Finetune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBkygCUDl7EQ"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"re_all\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYVwWtQLIl3X"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6Qq9igIlIW"
      },
      "source": [
        "tasks = [\n",
        "         ['chemprot', 'chemprot_re'],\n",
        "         ]\n",
        "output_dir = \"chemprot_pmc_v4\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAzDXwIlI16q"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  task = t[1]\n",
        "  input_file = task + '_predict_input.txt'\n",
        "  output_file = task + '_predict_output.txt'\n",
        "\n",
        "\n",
        "  # Write out the supplied questions to text files.\n",
        "  predict_inputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, input_file)\n",
        "  predict_outputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, output_dir , MODEL_SIZE, output_file)\n",
        "\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        # Select the most probable output token at each step.\n",
        "        # vocabulary=t5.data.SentencePieceVocabulary(vocab),\n",
        "\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + task)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "  # with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
        "  #   for q, a in zip(questions, f):\n",
        "  #     if q:\n",
        "  #       print(\"Q: \" + q)\n",
        "  #       print(\"A: \" + a)\n",
        "  #       print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBwOCWUTG-J9"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LvLpykXHQTu"
      },
      "source": [
        "import os\n",
        "checkpoint = 237400\n",
        "MODEL_SIZE = 'base'\n",
        "output_dir = os.path.join(output_dir, MODEL_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "325835ASQhAH"
      },
      "source": [
        "tasks = [\n",
        "         ['chemprot', 'chemprot_re', ''],\n",
        "         ]\n",
        "# for i in range(1,11):\n",
        "#   tasks.append(['euadr', 'euadr_%d'%i, output_dir + '/'])\n",
        "#   tasks.append(['GAD', 'GAD_%d'%i, output_dir + '/'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufW-7d5DHE0q"
      },
      "source": [
        "for task in tasks:\n",
        "  # t5_training/t5-data/bio_data/euadr/predicted_output\n",
        "  !gsutil -m cp gs://t5_training/t5-data/bio_data/{task[0]}/{output_dir}/{task[1]}_predict_output.txt-* . \n",
        "  !gsutil cp gs://t5_training/t5-data/bio_data/{task[0]}/{task[1]}_actual_output.txt . "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UA9r05mG7Mu"
      },
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, recall_score, precision_score, precision_recall_fscore_support\n",
        "import numpy as np\n",
        "import re\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "av7odWGzG7p6"
      },
      "source": [
        "def convert_RE_labels(filename):\n",
        "    labels = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            labels.append(line.strip().upper())\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuvjLJZDHDNQ"
      },
      "source": [
        "checkpoint = 1245000\n",
        "tasks = []\n",
        "# for i in range(1,11):\n",
        "#   tasks.append(['euadr', 'euadr_%d'%i, output_dir + '/'])\n",
        "#   tasks.append(['GAD', 'GAD_%d'%i, output_dir + '/'])\n",
        "tasks = [\n",
        "         ['chemprot', 'chemprot_re', ''],\n",
        "         ]\n",
        "total_f1 = 0\n",
        "total_precision = 0\n",
        "total_recall = 0\n",
        "# tasks = [\n",
        "#          ['chemprot', 'chemprot_re', ''],\n",
        "#          ]\n",
        "\n",
        "anchor_pred_labels = []\n",
        "anchor_actual_labels = []\n",
        "for task in tasks:\n",
        "    d = task[0]\n",
        "    t = task[1]\n",
        "    \n",
        "    pred_file = os.path.join('/content/', t +'_predict_output.txt-%s'%checkpoint)\n",
        "    actual_file = os.path.join('/content/', t + '_actual_output.txt')\n",
        "    \n",
        "    # pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "    # actual_file = 'test_raw.txt'\n",
        "    pred_labels = convert_RE_labels(pred_file)\n",
        "    actual_labels = convert_RE_labels(actual_file)\n",
        "#     print(pred_labels)\n",
        "#     print(actual_labels)\n",
        "#     pred_labels = np.zeros(len(actual_labels)).tolist()\n",
        "\n",
        "    \n",
        "    # f1score = f1_score(actual_labels, pred_labels, average='micro')\n",
        "    # recallscore = recall_score(actual_labels, pred_labels, average='micro')\n",
        "    # precisionscore = precision_score(actual_labels, pred_labels, average='micro')\n",
        "\n",
        "    # total_f1 += f1score\n",
        "    # total_recall += recallscore\n",
        "    # total_precision += precisionscore\n",
        "    # accuracy = accuracy_score(actual_labels, pred_labels)\n",
        "    # print(t , f1score, recallscore, precisionscore)\n",
        "    # break\n",
        "    \n",
        "#     f1score = f1_score(tmp_actual, tmp_pred)\n",
        "#     recallscore = recall_score(tmp_actual, tmp_pred)\n",
        "#     precisionscore = precision_score(tmp_actual, tmp_pred)\n",
        "    \n",
        "#     print(\"%s\\t Precision: %2f \\t Recall-score: %2f \\t F1-score: %2f \" % (t, precisionscore, recallscore, f1score))\n",
        "#     print(\"Accuracy score: %2f\" % accuracy_score(actual_labels, pred_labels))\n",
        "#     print(t)|\n",
        "    # print(\"Report:\", classification_report(actual_labels, pred_labels, digits=4, labels=labels))\n",
        "    print(\"Report %s:\"%t, classification_report(actual_labels, pred_labels, digits=4))\n",
        "    f1_score(y_pred=pred_labels, y_true=actual_labels, average='micro')\n",
        "    p,r,f,_ = precision_recall_fscore_support(y_pred=pred_labels, y_true=actual_labels)\n",
        "    results = dict()\n",
        "    results[\"f1 score\"] = f[1]\n",
        "    results[\"recall\"] = r[1]\n",
        "    results[\"precision\"] = p[1]\n",
        "    results[\"specificity\"] = r[0]     \n",
        "    print(t, results) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_service_account()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "qY5BckQL4Rjx",
        "outputId": "ec1e7311-cad7-495b-b0ff-af9a7fc20d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved credentials for gsutil@vietai-research.iam.gserviceaccount.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install t5"
      ],
      "metadata": {
        "id": "k5TLnSS24AhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp gs://scifive/finetune/chemprot/*_blurb.tsv ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU53g26Q5Gzn",
        "outputId": "971147f2-bb65-4d90-e756-947ba9be82f6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying gs://scifive/finetune/chemprot/dev_blurb.tsv...\n",
            "Copying gs://scifive/finetune/chemprot/test_blurb.tsv...\n",
            "Copying gs://scifive/finetune/chemprot/train_blurb.tsv...\n",
            "- [3 files][ 10.4 MiB/ 10.4 MiB]                                                \n",
            "Operation completed over 3 objects/10.4 MiB.                                     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "import functools\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "import argparse\n",
        "import tensorflow.compat.v1 as tf\n",
        "import t5\n",
        "from t5.models import MtfModel\n",
        "import os\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D90NOjLO5VUJ",
        "outputId": "505521e0-a932-4356-ffdc-305ac1349d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "TPU_ADDRESS = tpu.get_master()\n",
        "tf.disable_v2_behavior()\n",
        "TPU_TOPOLOGY = 'v2-8'\n",
        "\n",
        "\n",
        "print(f\"TPU Address {TPU_ADDRESS}\")\n",
        "ON_CLOUD = True\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n",
        "\n",
        "\n",
        "## Bypass an error\n",
        "tf.compat.v1.flags.DEFINE_string('f','','')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhxxt4Td5YSH",
        "outputId": "28e97ac4-b2ed-43be-aaee-b16c2a673a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TPU Address None\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<absl.flags._flagvalues.FlagHolder at 0x7fb84b938940>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAqT4VQA33Al",
        "outputId": "2561e600-1e2b-4c59-b33e-3a7b1eff4fe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:system_path_file_exists:gs://scifive/models/pubmed/base/operative_config.gin\n",
            "ERROR:root:Path not found: gs://scifive/models/pubmed/base/operative_config.gin\n",
            "INFO:root:Skipping import of unknown module `t5.data.sentencepiece_vocabulary` (skip_unknown=True).\n",
            "eval_on_tpu ignored because use_tpu is False.\n",
            "From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/training_util.py:396: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:absl:Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n",
            "From /usr/local/lib/python3.8/dist-packages/seqio/dataset_providers.py:1537: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.sample_from_datasets(...)`.\n",
            "Using default tf glorot_uniform_initializer for variable encoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "Using default tf glorot_uniform_initializer for variable decoder/block_000/layer_000/SelfAttention/relative_attention_bias  The initialzer will guess the input and output dimensions  based on dimension order.\n",
            "From /usr/local/lib/python3.8/dist-packages/mesh_tensorflow/transformer/utils.py:939: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "task = 'chemprot'\n",
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/chemprot/train_blurb.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "                        ]\n",
        "          )\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def preprocessor(ds):\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": {task}..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "            tf.strings.join(\n",
        "                [f\"{task}: \", ex[\"input\"]]),\n",
        "        \"targets\": ex[\"target\"]\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "t5.data.TaskRegistry.remove(task)\n",
        "t5.data.TaskRegistry.add(\n",
        "    task,\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    text_preprocessor=[preprocessor],\n",
        "    metric_fns=[],\n",
        ")\n",
        "\n",
        "t5.data.MixtureRegistry.remove(\"all\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"all\",\n",
        "    [task],\n",
        "     default_rate=1.0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_SIZE = 'base'\n",
        "\n",
        "# Set parallelism and batch size to fit on v3-8 TPU (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (4, 256, 8),\n",
        "    \"large\": (8, 256, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "PRETRAINED_DIR = f\"gs://scifive/models/pubmed/{MODEL_SIZE}/\"\n",
        "\n",
        "\n",
        "# change to your own MODEL_DIR \n",
        "MODEL_DIR = \"gs://YOURMODEL_/tmp/scifive_pubmed_base_chemprot_blurb\"\n",
        "\n",
        "model = MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 256, \"targets\": 8},\n",
        "    learning_rate_schedule=0.001,\n",
        "    iterations_per_loop=100,\n",
        "    save_checkpoints_steps=2000,\n",
        "    keep_checkpoint_max=5,\n",
        ")\n",
        "\n",
        "FINETUNE_STEPS = 25000\n",
        "model.finetune(\n",
        "   mixture_or_task_name=\"all\",\n",
        "   pretrained_model_dir=PRETRAINED_DIR,\n",
        "   finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "metadata": {
        "id": "42NMdnNJYQs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_file = \"test\" or \"dev\"\n",
        "\n",
        "print(\"======starting predicting=======\")\n",
        "with open(f\"{predicted_file}_blurb.tsv\") as file:\n",
        "  with open(\"predict_input.txt\", \"w\") as outfile:\n",
        "    with open(\"actual_output.txt\", \"w\") as outfile1:    \n",
        "      for line in file:\n",
        "        line = line.strip().split('\\t')\n",
        "        outfile.write(f\"{task}: {line[0]}\\n\")\n",
        "        outfile1.write(f\"{line[1]}\\n\")\n",
        "\n",
        "input_file = \"predict_input.txt\"\n",
        "output_file = \"predict_output.txt\"\n",
        "\n",
        "checkpoints = [int(x.replace('.index', '').split('-')[-1]) for x in tf.io.gfile.glob(MODEL_DIR +'/*ckpt*.index')]\n",
        "checkpoints.sort()\n",
        "checkpoints = checkpoints[-5:]\n",
        "for checkpoint in checkpoints:\n",
        "    if os.path.exists(f'{output_file}-{checkpoint}'):\n",
        "        print(f\"skipping {checkpoint}\")\n",
        "        continue\n",
        "    with tf_verbosity_level('ERROR'):\n",
        "      model.batch_size = 8 \n",
        "      model.predict(\n",
        "          input_file=input_file,\n",
        "          output_file=output_file,\n",
        "          checkpoint_steps=checkpoint,\n",
        "          temperature=0,\n",
        "    )"
      ],
      "metadata": {
        "id": "U7WSNrpi5QsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Evaluations script similiar to https://github.com/michiyasunaga/LinkBERT\n",
        "\n",
        "TP = 0\n",
        "P_total = 0\n",
        "L_total = 0\n",
        "for p,a in zip(predictions, references):\n",
        "    p = p.strip()\n",
        "    a = a.strip()\n",
        "    if p == a and p != \"0\":\n",
        "        TP += 1\n",
        "    if p != \"0\":\n",
        "        P_total += 1\n",
        "    if a != \"0\":\n",
        "        L_total += 1\n",
        "P = TP / P_total if P_total else 0\n",
        "R = TP / L_total if L_total else 0\n",
        "F1 = 2 * P * R / (P + R) if (P + R) else 0\n",
        "F1"
      ],
      "metadata": {
        "id": "TcWZNJTXPcIV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
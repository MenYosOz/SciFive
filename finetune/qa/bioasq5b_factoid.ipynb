{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_bio_finetune_bioasq5b_factoid",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOisOJSerBcX"
      },
      "source": [
        "# All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxQv5QTPqMS"
      },
      "source": [
        "## Set Up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hglvOHLCd1sg"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7wZkDsohpUE"
      },
      "source": [
        "## Set UP TPU Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE1ujiZOhlyo"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrToSKD9o671"
      },
      "source": [
        "## 5b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L77djlkAo673"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/bioasq5b/bioasq_5b_train_1.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/bio_data/bioasq5b/bioasq_5b_test.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4e5190uo676"
      },
      "source": [
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bioasq5b: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxX9sBsfo67_"
      },
      "source": [
        "t5.data.TaskRegistry.remove('bioasq5b')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"bioasq5b\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lvldl5REml5o"
      },
      "source": [
        "nq_task = t5.data.TaskRegistry.get(\"bioasq5b\")\n",
        "ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 128, \"targets\": 128})\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR585EBykpjG"
      },
      "source": [
        "## Dataset Mixture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odi8I-pYkn4j"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"bioasqb\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"bioasqb\",\n",
        "    [\"bioasq5b\"],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11UkumTlDMh"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vtASB-lCBQ"
      },
      "source": [
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v1\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "MODEL_DIR = \"gs://t5_training/models/bio/bioasq5b_pmc_v1\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "\n",
        "\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128*2, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length={\"inputs\": 512, \"targets\": 32},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xoj_Vhj0HLDZ"
      },
      "source": [
        "## Finetune\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaqkGlbDHMJe"
      },
      "source": [
        "FINETUNE_STEPS = 35000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"bioasqb\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpONPoc8HObH"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnZ1QVdGHPTG"
      },
      "source": [
        "year = 5\n",
        "output_dir = 'bioasq5b_pmc_v1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmBkVmQAHdmk"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "# for year in range(4,7):\n",
        "for batch in range (1,6):\n",
        "  task = \"%dB%d\"%(year, batch)\n",
        "  dir = \"bioasq%db\"%(year)\n",
        "  input_file = task + '_factoid_predict_input.txt'\n",
        "  output_file = task + '_predict_output.txt'\n",
        "\n",
        "  predict_inputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, 'eval_data', input_file)\n",
        "  print(predict_inputs_path)\n",
        "  predict_outputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, output_dir, MODEL_SIZE, output_file)\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        # Select the most probable output token at each step.\n",
        "        temperature=0,\n",
        "    )\n",
        "  print(\"Predicted task : \" + task)\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "\n",
        "  # t5_training/t5-data/bio_data/bioasq4b/eval_data/4B1_factoid_predict_input.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
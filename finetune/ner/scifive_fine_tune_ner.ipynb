{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "bio_t5_fine_tune_ner.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgxQv5QTPqMS"
      },
      "source": [
        "## Set Up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hglvOHLCd1sg"
      },
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -q t5\n",
        "\n",
        "import functools\n",
        "import os\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "import gin\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7wZkDsohpUE"
      },
      "source": [
        "## Set UP TPU Runtime"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE1ujiZOhlyo"
      },
      "source": [
        "ON_CLOUD = True\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v3-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU zdetection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "  auth.authenticate_user()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "gin.parse_config_file(\n",
        "        'gs://t5_training/t5-data/config/pretrained_models_google_base_operative_config.gin'\n",
        ")\n",
        "# gin.bind_parameter(\"SentencePieceVocabulary.extra_ids\", 100)\n",
        "\n",
        "# DEFAULT_OUTPUT_FEATURES = {\n",
        "#     \"inputs\": Feature(\n",
        "#         vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n",
        "#     \"targets\": Feature(vocabulary=get_default_vocabulary(), add_eos=True)\n",
        "# }\n",
        "vocab = \"gs://t5_training/models/spm/t5_bio_spm_small.model\"\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  og_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(og_level)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TNuZ7UmiUqA"
      },
      "source": [
        "## Register NER Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjtB4YHVkeMC"
      },
      "source": [
        "### NCBI NER Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpwpnxiFgcnC"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/NCBI-disease/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/NCBI-disease/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXaiyZjpgeM5"
      },
      "source": [
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"ncbi_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Dbe7-xGjxHG"
      },
      "source": [
        "t5.data.TaskRegistry.remove('ncbi_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"ncbi_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "                ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zRrOloPgiZh"
      },
      "source": [
        "### BC5CDR Chemical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7LEzEtNgsby"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC5CDR-chem/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC5CDR-chem/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bc5cdr_chem_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  \n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snDTIAHyg-dD"
      },
      "source": [
        "t5.data.TaskRegistry.remove('bc5cdr_chem_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"bc5cdr_chem_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3wH4dH-hvX9"
      },
      "source": [
        "### BC5CDR Disease"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8g32QO-hzAb"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC5CDR-disease/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC5CDR-disease/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bc5cdr_disease_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muftz4yGhzZs"
      },
      "source": [
        "t5.data.TaskRegistry.remove('bc5cdr_disease_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"bc5cdr_disease_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab)),\n",
        "\n",
        "    # output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54JePoV5HX_v"
      },
      "source": [
        "### BC2GM NER Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7mkg_SMHeH1"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC2GM/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC2GM/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXgdTdIlHziM"
      },
      "source": [
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bc2gm_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGupplxQH3n3"
      },
      "source": [
        "t5.data.TaskRegistry.remove('bc2gm_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"bc2gm_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1lO5V8GH83I"
      },
      "source": [
        "###  BC4CHEMD NER Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs5MXCYxIBt2"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC4CHEMD/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/BC4CHEMD/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"bc4chemd_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8N3pQmlTIUEn"
      },
      "source": [
        "t5.data.TaskRegistry.remove('bc4chemd_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"bc4chemd_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_UdTMYOIoxc"
      },
      "source": [
        "### JNLPBA NER Task\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7C0K9w3tIqyL"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "             'gs://scifive/finetune/JNLPBA/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/JNLPBA/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"jnlpba_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Taw5eIXDIzUT"
      },
      "source": [
        "t5.data.TaskRegistry.remove('jnlpba_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"jnlpba_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OZmNoZjJV7y"
      },
      "source": [
        "### LINNAEUS NER Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoXctyN1JaHU"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/linnaeus/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/linnaeus/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"linnaeus_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z4Mq3cNNTSn"
      },
      "source": [
        "t5.data.TaskRegistry.remove('linnaeus_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"linnaeus_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPFXphWlqMvP"
      },
      "source": [
        "### S800 NER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCfds90XqQNv"
      },
      "source": [
        "def dumping_dataset(split, shuffle_files = False):\n",
        "    del shuffle_files\n",
        "    if split == 'train':\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/s800/train.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    else:\n",
        "      ds = tf.data.TextLineDataset(\n",
        "            [\n",
        "            'gs://scifive/finetune/s800/test.tsv_cleaned.tsv',\n",
        "            ]\n",
        "          )\n",
        "    # Split each \"<t1>\\t<t2>\" example into (input), target) tuple.\n",
        "    ds = ds.map(\n",
        "        functools.partial(tf.io.decode_csv, record_defaults=[\"\", \"\"],\n",
        "                          field_delim=\"\\t\", use_quote_delim=False),\n",
        "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "    # Map each tuple to a {\"input\": ... \"target\": ...} dict.\n",
        "    ds = ds.map(lambda *ex: dict(zip([\"input\", \"target\"], ex)))\n",
        "    return ds\n",
        "\n",
        "def ner_preprocessor(ds):\n",
        "  def normalize_text(text):\n",
        "    \"\"\"Lowercase and remove quotes from a TensorFlow string.\"\"\"\n",
        "    return text\n",
        "\n",
        "  def to_inputs_and_targets(ex):\n",
        "    \"\"\"Map {\"inputs\": ..., \"targets\": ...}->{\"inputs\": ner..., \"targets\": ...}.\"\"\"\n",
        "    return {\n",
        "        \"inputs\":\n",
        "             tf.strings.join(\n",
        "                 [\"s800_ner: \", normalize_text(ex[\"input\"])]),\n",
        "        \"targets\": normalize_text(ex[\"target\"])\n",
        "    }\n",
        "  return ds.map(to_inputs_and_targets, \n",
        "                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "print(\"A few raw validation examples...\")\n",
        "for ex in tfds.as_numpy(dumping_dataset(\"train\").take(5)):\n",
        "  print(ex)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJ54zvOBqZ34"
      },
      "source": [
        "t5.data.TaskRegistry.remove('s800_ner')\n",
        "t5.data.TaskRegistry.add(\n",
        "    \"s800_ner\",\n",
        "    # Supply a function which returns a tf.data.Dataset.\n",
        "    dataset_fn=dumping_dataset,\n",
        "    splits=[\"train\", \"validation\"],\n",
        "    # Supply a function which preprocesses text from the tf.data.Dataset.\n",
        "    text_preprocessor=[ner_preprocessor],\n",
        "    # Lowercase targets before computing metrics.\n",
        "    postprocess_fn=t5.data.postprocessors.lower_text, \n",
        "    # We'll use accuracy as our evaluation metric.\n",
        "    metric_fns=[t5.evaluation.metrics.accuracy, \n",
        "               t5.evaluation.metrics.sequence_accuracy, \n",
        "               ],\n",
        "    output_features=t5.data.Feature(vocabulary=t5.data.SentencePieceVocabulary(vocab))\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDCDXK9Eycu8"
      },
      "source": [
        "## Mixtures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odi8I-pYkn4j"
      },
      "source": [
        "t5.data.MixtureRegistry.remove(\"ner_all\")\n",
        "t5.data.MixtureRegistry.add(\n",
        "    \"ner_all\",\n",
        "    [\n",
        "     \"ncbi_ner\", \n",
        "     \"bc5cdr_disease_ner\", \n",
        "     \"bc5cdr_chem_ner\", \n",
        "     'bc4chemd_ner', \n",
        "     'bc2gm_ner', \n",
        "     'jnlpba_ner', \n",
        "     'linnaeus_ner', \n",
        "     's800_ner'\n",
        "     ],\n",
        "     default_rate=1.0\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i11UkumTlDMh"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1vtASB-lCBQ"
      },
      "source": [
        "\n",
        "\n",
        "# Using pretrained_models from wiki + books\n",
        "MODEL_SIZE = \"base\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5-data/pretrained_models\"\n",
        "# BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v2\"\n",
        "BASE_PRETRAINED_DIR = \"gs://t5_training/models/bio/pmc_v2\"\n",
        "PRETRAINED_DIR = os.path.join(BASE_PRETRAINED_DIR, MODEL_SIZE)\n",
        "MODEL_DIR = \"gs://t5_training/models/bio/ner_all_pmc_v2\"\n",
        "MODEL_DIR = os.path.join(MODEL_DIR, MODEL_SIZE)\n",
        "# Set parallelism and batch size to fit on v2-8 TPU (if possible).\n",
        "# Limit number of checkpoints to fit within 5GB (if possible).\n",
        "model_parallelism, train_batch_size, keep_checkpoint_max = {\n",
        "    \"small\": (1, 256, 16),\n",
        "    \"base\": (2, 128*2, 8),\n",
        "    \"large\": (8, 64, 4),\n",
        "    \"3B\": (8, 16, 1),\n",
        "    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n",
        "\n",
        "tf.io.gfile.makedirs(MODEL_DIR)\n",
        "# The models from our paper are based on the Mesh Tensorflow Transformer.\n",
        "model = t5.models.MtfModel(\n",
        "    model_dir=MODEL_DIR,\n",
        "    tpu=TPU_ADDRESS,\n",
        "    tpu_topology=TPU_TOPOLOGY,\n",
        "    model_parallelism=model_parallelism,\n",
        "    batch_size=train_batch_size,\n",
        "    sequence_length = {'inputs': 256, 'targets': 256},\n",
        "    learning_rate_schedule=0.001,\n",
        "    save_checkpoints_steps=1000,\n",
        "    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n",
        "    iterations_per_loop=100,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAnCs6eQJj9I"
      },
      "source": [
        "vocabulary1 = t5.data.get_mixture_or_task(\"ner_all\").get_vocabulary()\n",
        "print(\" ******** vocabulary1\")\n",
        "print(vocabulary1)\n",
        "print(vocabulary1.vocab_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBu7B_Vdl5BB"
      },
      "source": [
        "## Finetune "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJSRtli1UlQj"
      },
      "source": [
        "if ON_CLOUD:\n",
        "  %reload_ext tensorboard\n",
        "  import tensorboard as tb\n",
        "tb.notebook.start(\"--logdir \" + MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjNd4BIKl06z"
      },
      "source": [
        "FINETUNE_STEPS = 45000\n",
        "\n",
        "model.finetune(\n",
        "    mixture_or_task_name=\"ner_all\",\n",
        "    pretrained_model_dir=PRETRAINED_DIR,\n",
        "    finetune_steps=FINETUNE_STEPS\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynIhfRMDm3pO"
      },
      "source": [
        "## Export Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43yafDz9l7QN"
      },
      "source": [
        "export_dir = os.path.join(MODEL_DIR, \"export\")\n",
        "\n",
        "model.batch_size = 1 # make one prediction per call\n",
        "saved_model_path = model.export(\n",
        "    export_dir,\n",
        "    checkpoint_step=-1,  # use most recent\n",
        "    beam_size=1,  # no beam search\n",
        "    temperature=1.0,  # sample according to predicted distribution\n",
        ")\n",
        "print(\"Model saved to:\", saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qh0hQPtPNgkJ"
      },
      "source": [
        "## Load Saved Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1FjOX1K6x2J"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "print(tf.__version__)\n",
        "def load_predict_fn(model_path):\n",
        "  if tf.executing_eagerly():\n",
        "    print(\"Loading SavedModel in eager mode.\")\n",
        "    imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "    return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
        "  else:\n",
        "    print(\"Loading SavedModel in tf 1.x graph mode.\")\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    meta_graph_def = tf.compat.v1.saved_model.load(sess, [\"serve\"], model_path)\n",
        "    signature_def = meta_graph_def.signature_def[\"serving_default\"]\n",
        "    return lambda x: sess.run(\n",
        "        fetches=signature_def.outputs[\"outputs\"].name, \n",
        "        feed_dict={signature_def.inputs[\"input\"].name: x}\n",
        "    )\n",
        "\n",
        "predict_fn = load_predict_fn(saved_model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCGXX-yvyHTF"
      },
      "source": [
        "imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "test = lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeN89QnANzMe"
      },
      "source": [
        "## Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0RYeWZPLUb6c"
      },
      "source": [
        "tasks = [\n",
        "         ['NCBI-disease', \"ncbi_ner\"], \n",
        "         ['BC5CDR-disease', \"bc5cdr_disease_ner\"], \n",
        "         ['BC5CDR-chem', \"bc5cdr_chem_ner\"], \n",
        "         ['BC4CHEMD', 'bc4chemd_ner'], \n",
        "         ['BC2GM', 'bc2gm_ner'], \n",
        "         ['JNLPBA', 'jnlpba_ner'], \n",
        "         ['linnaeus', 'linnaeus_ner'], \n",
        "         ['s800', 's800_ner']\n",
        "         ]\n",
        "output_dir = 'ner_all_pubmed_v2'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZxUTGJmPdYO"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "# question_1 = \"Emerin is a nuclear membrane protein which is missing or defective in Emery-Dreifuss muscular dystrophy (EDMD). It is one member of a family of lamina-associated proteins which includes LAP1, LAP2 and lamin B receptor (LBR). A panel of 16 monoclonal antibodies (mAbs) has been mapped to six specific sites throughout the emerin molecule using phage-displayed peptide libraries and has been used to localize emerin in human and rabbit heart. Several mAbs against different emerin epitopes did not recognize intercalated discs in the heart, though they recognized cardiomyocyte nuclei strongly, both at the rim and in intranuclear spots or channels. A polyclonal rabbit antiserum against emerin did recognize both nuclear membrane and intercalated discs but, after affinity purification against a pure-emerin band on a western blot, it stained only the nuclear membrane. These results would not be expected if immunostaining at intercalated discs were due to a product of the emerin gene and, therefore, cast some doubt upon the hypothesis that cardiac defects in EDMD are caused by absence of emerin from intercalated discs. Although emerin was abundant in the membranes of cardiomyocyte nuclei, it was absent from many non-myocyte cells in the heart. This distribution of emerin was similar to that of lamin A, a candidate gene for an autosomal form of EDMD. In contrast, lamin B1 was absent from cardiomyocyte nuclei, showing that lamin B1 is not essential for localization of emerin to the nuclear lamina. Lamin B1 is also almost completely absent from skeletal muscle nuclei. In EDMD, the additional absence of lamin B1 from heart and skeletal muscle nuclei which already lack emerin may offer an alternative explanation of why these tissues are particularly affected..\" \n",
        "# question_2 = \"Molecular analysis of the APC gene in 205 families: extended genotype-phenotype correlations in FAP and evidence for the role of APC amino acid changes in colorectal cancer predisposition.\" \n",
        "# question_3 = \"Who are the 4 members of The Beatles?\" \n",
        "# question_4 = \"How many teeth do humans have?\"\n",
        "\n",
        "# questions = [question_2]\n",
        "\n",
        "\n",
        "for t in tasks:\n",
        "  dir = t[0]\n",
        "  task = t[1]\n",
        "  input_file = task + '_predict_input.txt'\n",
        "  output_file = task + '_predict_output.txt'\n",
        "\n",
        "\n",
        "  # Write out the supplied questions to text files.\n",
        "  predict_inputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, input_file)\n",
        "  predict_outputs_path = os.path.join('gs://t5_training/t5-data/bio_data', dir, output_dir, MODEL_SIZE, output_file)\n",
        "  # Manually apply preprocessing by prepending \"triviaqa question:\".\n",
        "\n",
        "  # Ignore any logging so that we only see the model's answers to the questions.\n",
        "  with tf_verbosity_level('ERROR'):\n",
        "    model.batch_size = 8  # Min size for small model on v2-8 with parallelism 1.\n",
        "    model.predict(\n",
        "        input_file=predict_inputs_path,\n",
        "        output_file=predict_outputs_path,\n",
        "        # Select the most probable output token at each step.\n",
        "        vocabulary=t5.data.SentencePieceVocabulary(vocab),\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "  # The output filename will have the checkpoint appended so we glob to get \n",
        "  # the latest.\n",
        "  prediction_files = sorted(tf.io.gfile.glob(predict_outputs_path + \"*\"))\n",
        "  print(\"Predicted task : \" + task)\n",
        "  print(\"\\nPredictions using checkpoint %s:\\n\" % prediction_files[-1].split(\"-\")[-1])\n",
        "  # with tf.io.gfile.GFile(prediction_files[-1]) as f:\n",
        "  #   for q, a in zip(questions, f):\n",
        "  #     if q:\n",
        "  #       print(\"Q: \" + q)\n",
        "  #       print(\"A: \" + a)\n",
        "  #       print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXUsUBeZ2-m"
      },
      "source": [
        "## Scoring"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pv8k4M1AZ1V1"
      },
      "source": [
        "!pip install seqeval\n",
        "import nltk\n",
        "from seqeval.metrics import f1_score, accuracy_score, classification_report, recall_score, precision_score\n",
        "import re\n",
        "import os\n",
        "tasks = [\n",
        "         ['NCBI-disease', \"ncbi_ner\"], \n",
        "         ['BC5CDR-disease', \"bc5cdr_disease_ner\"], \n",
        "         ['BC5CDR-chem', \"bc5cdr_chem_ner\"], \n",
        "         ['BC4CHEMD', 'bc4chemd_ner'], \n",
        "         ['BC2GM', 'bc2gm_ner'], \n",
        "         ['JNLPBA', 'jnlpba_ner'], \n",
        "         ['linnaeus', 'linnaeus_ner'], \n",
        "         ['s800', 's800_ner']\n",
        "         ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvwgJlJn3Tlw"
      },
      "source": [
        "checkpoint = 249600\n",
        "for t in tasks:\n",
        "  !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{output_dir}/base/{t[1]}_predict_output.txt-* .\n",
        "  # !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{t[1]}_predict_output.txt-* .\n",
        "  # t5_training/t5-data/bio_data/BC4CHEMD/predicted_output_original_model/base\n",
        "  !gsutil cp gs://t5_training/t5-data/bio_data/{t[0]}/{t[1]}_actual_output.txt .\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWnAELJ-aAm0"
      },
      "source": [
        "def convert_BIO_labels(filename):\n",
        "    result_labels = []\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        cnt = 0\n",
        "        for line in file:\n",
        "            line = re.sub(r'\\*(\\w+)', r'\\1*', line)\n",
        "            tokens = re.sub(r'[!\"#$%&\\'()+,-.:;<=>?@[\\\\\\]^_`{\\|}~⁇]', ' ', line.strip()).split()\n",
        "            seq_label = []\n",
        "            start_entity = 0\n",
        "            entity_type = 'O'\n",
        "            for idx, token in enumerate(tokens):\n",
        "                if token.endswith('*'):\n",
        "                    start_entity += 1 if (start_entity == 0 or token[:-1] != entity_type) else -1\n",
        "                    entity_type = token[:-1]\n",
        "                else:\n",
        "                    if start_entity == 0:\n",
        "                        seq_label.append('O')\n",
        "                        entity_type = 'O'\n",
        "                    elif start_entity < 0:\n",
        "                        raise \"Something errors\"\n",
        "                    else:\n",
        "                        if tokens[idx - 1].endswith('*'):\n",
        "                            seq_label.append('B-' + entity_type.upper())\n",
        "                        else:\n",
        "                            seq_label.append('I-' + entity_type.upper())\n",
        "\n",
        "            result_labels.append(seq_label)\n",
        "            cnt += 1\n",
        "#             if cnt % 100 == 0:\n",
        "#                 print('Processed %d sentences' % cnt)\n",
        "    return result_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMjBWpYNaH0n",
        "collapsed": true
      },
      "source": [
        "# pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "# actual_file = 'test_raw.txt'\n",
        "# pred_file = 'data/ncbi/t5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "# actual_file = 'data/ncbi/test_raw.txt'\n",
        "checkpoint = 237400\n",
        "for task in tasks:\n",
        "    d = task[0]\n",
        "    t = task[1]\n",
        "    \n",
        "    pred_file = os.path.join(t + '_predict_output.txt-%d'%checkpoint)\n",
        "    actual_file = os.path.join(t + '_actual_output.txt')\n",
        "    \n",
        "    # pred_file = 't5-data_bio_data_NCBI_NER_predict_outputs_1603446926.txt-1017500'\n",
        "    # actual_file = 'test_raw.txt'\n",
        "    pred_labels = convert_BIO_labels(pred_file)\n",
        "    actual_labels = convert_BIO_labels(actual_file)\n",
        "    ;print\n",
        "    for i, (a, b) in enumerate(zip(pred_labels, actual_labels)):\n",
        "        len_a = len(a)\n",
        "        len_b = len(b)\n",
        "        \n",
        "        if len_a > len_b:\n",
        "            pred_labels[i] = pred_labels[i][:len_b]\n",
        "        elif len_a < len_b:\n",
        "            pred_labels[i] = pred_labels[i] + ['PAD'] * (len_b - len_a)\n",
        "            \n",
        "    f1score = f1_score(actual_labels, pred_labels)\n",
        "    recallscore = recall_score(actual_labels, pred_labels)\n",
        "    precisionscore = precision_score(actual_labels, pred_labels)\n",
        "    \n",
        "#     f1score = f1_score(tmp_actual, tmp_pred)\n",
        "#     recallscore = recall_score(tmp_actual, tmp_pred)\n",
        "#     precisionscore = precision_score(tmp_actual, tmp_pred)\n",
        "    \n",
        "#     print(\"%s\\t Precision: %2f \\t Recall-score: %2f \\t F1-score: %2f \" % (t, precisionscore, recallscore, f1score))\n",
        "#     print(\"Accuracy score: %2f\" % accuracy_score(tmp_actual, tmp_pred))\n",
        "    print(t, 'f1-score', f1score)\n",
        "    print('recallscore', recallscore)\n",
        "    print('precisionscore', precisionscore)\n",
        "    # print(\"Report:\", classification_report(actual_labels, pred_labels, digits=4))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
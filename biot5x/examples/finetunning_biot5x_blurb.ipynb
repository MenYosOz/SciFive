{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3950e320-16c4-48ab-bdc7-21d406635524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'SciFive'...\n",
      "remote: Enumerating objects: 436, done.\u001b[K\n",
      "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
      "remote: Compressing objects: 100% (123/123), done.\u001b[K\n",
      "remote: Total 436 (delta 129), reused 90 (delta 55), pack-reused 251\u001b[K\n",
      "Receiving objects: 100% (436/436), 15.84 MiB | 2.12 MiB/s, done.\n",
      "Resolving deltas: 100% (216/216), done.\n",
      "Updating files: 100% (120/120), done.\n"
     ]
    }
   ],
   "source": [
    "!rm -r SciFive\n",
    "!git clone https://github.com/justinphan3110/SciFive.git\n",
    "!cp -r SciFive/biot5x ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a8d42-7851-433c-9b77-ebf5c23648be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Install JAX for GPU\n",
    "!pip install jaxlib==0.4.1+cuda11.cudnn86 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "## Install T5X and dependencies\n",
    "!cd biot5x && python3 setup.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa22a3c-f750-486f-8e17-0106600a7c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download BioT5X base model\n",
    "!mkdir biot5x_base\n",
    "!gsutil -m cp -r gs://scifive/biot5x/base/* biot5x_base/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bd1f3-9c3c-497c-8e78-9f10d7c50bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### NER: NCBI_disease ###############################\n",
    "\n",
    "model_size = 'base'\n",
    "task = 'NCBI_disease'\n",
    "train_file = f'biot5x/data/{task}/train_blurb.tsv'\n",
    "test_file = f'biot5x/data/{task}/test_blurb.tsv'\n",
    "dev_file = f'biot5x/data/{task}/dev_blurb.tsv'\n",
    "\n",
    "model_dir = f'out/{task}/{model_size}_{task}'\n",
    "pretrained_path=f'biot5x_{model_size}'\n",
    "gin_file = f'biot5x/configs/biot5x/finetune/base/{task}_blurb.gin'\n",
    "\n",
    "metric = 'ner_metric'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "%run 'biot5x/src/finetune_biot5x.py' \\\n",
    "  --gin_file=\"{gin_file}\" \\\n",
    "  --gin.MODEL_DIR=\"'{model_dir}'\" \\\n",
    "  --gin.INITIAL_CHECKPOINT_PATH=\"'{pretrained_path}'\" \\\n",
    "  --gin.BATCH_SIZE='{batch_size}' \\\n",
    "  --task=\"{task}\" \\\n",
    "  --metric=\"{metric}\" \\\n",
    "  --train_file=\"{train_file}\" \\\n",
    "  --predict_file=\"{test_file}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818a87a3-c675-4661-b6f0-90d54497c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### NER: BC5CDR_disease ###############################\n",
    "model_size = 'base'\n",
    "task = 'BC5CDR_disease'\n",
    "train_file = f'biot5x/data/{task}/train_blurb.tsv'\n",
    "test_file = f'biot5x/data/{task}/test_blurb.tsv'\n",
    "dev_file = f'biot5x/data/{task}/dev_blurb.tsv'\n",
    "\n",
    "model_dir = f'out/{task}/{model_size}_{task}'\n",
    "pretrained_path=f'biot5x_{model_size}'\n",
    "gin_file = f'biot5x/configs/biot5x/finetune/base/{task}_blurb.gin'\n",
    "\n",
    "metric = 'ner_metric'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "%run 'biot5x/src/finetune_biot5x.py' \\\n",
    "  --gin_file=\"{gin_file}\" \\\n",
    "  --gin.MODEL_DIR=\"'{model_dir}'\" \\\n",
    "  --gin.INITIAL_CHECKPOINT_PATH=\"'{pretrained_path}'\" \\\n",
    "  --gin.BATCH_SIZE='{batch_size}' \\\n",
    "  --task=\"{task}\" \\\n",
    "  --metric=\"{metric}\" \\\n",
    "  --train_file=\"{train_file}\" \\\n",
    "  --predict_file=\"{test_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dacedc-27bf-4b15-ab28-1e67ebb6c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### NER: BC5CDR_disease ###############################\n",
    "model_size = 'base'\n",
    "task = 'JNLPBA'\n",
    "train_file = f'biot5x/data/{task}/train_blurb.tsv'\n",
    "test_file = f'biot5x/data/{task}/test_blurb.tsv'\n",
    "dev_file = f'biot5x/data/{task}/dev_blurb.tsv'\n",
    "\n",
    "model_dir = f'out/{task}/{model_size}_{task}'\n",
    "pretrained_path=f'biot5x_{model_size}'\n",
    "gin_file = f'biot5x/configs/biot5x/finetune/base/{task}_blurb.gin'\n",
    "\n",
    "metric = 'ner_metric'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "%run 'biot5x/src/finetune_biot5x.py' \\\n",
    "  --gin_file=\"{gin_file}\" \\\n",
    "  --gin.MODEL_DIR=\"'{model_dir}'\" \\\n",
    "  --gin.INITIAL_CHECKPOINT_PATH=\"'{pretrained_path}'\" \\\n",
    "  --gin.BATCH_SIZE='{batch_size}' \\\n",
    "  --task=\"{task}\" \\\n",
    "  --metric=\"{metric}\" \\\n",
    "  --train_file=\"{train_file}\" \\\n",
    "  --predict_file=\"{test_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf32d1-8c9f-4b9c-a0f0-0b66d67b1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### NER: BC5CDR_chem ###############################\n",
    "model_size = 'base'\n",
    "task = 'BC5CDR_chem'\n",
    "train_file = f'biot5x/data/{task}/train_blurb.tsv'\n",
    "test_file = f'biot5x/data/{task}/test_blurb.tsv'\n",
    "dev_file = f'biot5x/data/{task}/dev_blurb.tsv'\n",
    "\n",
    "model_dir = f'out/{task}/{model_size}_{task}'\n",
    "pretrained_path=f'biot5x_{model_size}'\n",
    "gin_file = f'biot5x/configs/biot5x/finetune/base/{task}_blurb.gin'\n",
    "\n",
    "metric = 'ner_metric'\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "%run 'biot5x/src/finetune_biot5x.py' \\\n",
    "  --gin_file=\"{gin_file}\" \\\n",
    "  --gin.MODEL_DIR=\"'{model_dir}'\" \\\n",
    "  --gin.INITIAL_CHECKPOINT_PATH=\"'{pretrained_path}'\" \\\n",
    "  --gin.BATCH_SIZE='{batch_size}' \\\n",
    "  --task=\"{task}\" \\\n",
    "  --metric=\"{metric}\" \\\n",
    "  --train_file=\"{train_file}\" \\\n",
    "  --predict_file=\"{test_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbab2ec7-0d0d-48a0-8644-57d1b48351cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 04:01:40.686106: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-29 04:01:40.686212: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "2023-01-29 04:01:40.686223: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "I0129 04:01:48.815579 140222066542400 resource_reader.py:50] system_path_file_exists:configs/biot5x/base_finetune_single_gpu.gin\n",
      "E0129 04:01:48.816699 140222066542400 resource_reader.py:55] Path not found: configs/biot5x/base_finetune_single_gpu.gin\n",
      "I0129 04:01:48.817160 140222066542400 resource_reader.py:50] system_path_file_exists:./configs/biot5x/base_finetune_single_gpu.gin\n",
      "E0129 04:01:48.817557 140222066542400 resource_reader.py:55] Path not found: ./configs/biot5x/base_finetune_single_gpu.gin\n",
      "I0129 04:01:48.830865 140222066542400 resource_reader.py:50] system_path_file_exists:configs/t5/t5_1_0/base.gin\n",
      "E0129 04:01:48.831506 140222066542400 resource_reader.py:55] Path not found: configs/t5/t5_1_0/base.gin\n",
      "I0129 04:01:48.831938 140222066542400 resource_reader.py:50] system_path_file_exists:./configs/t5/t5_1_0/base.gin\n",
      "E0129 04:01:48.832351 140222066542400 resource_reader.py:55] Path not found: ./configs/t5/t5_1_0/base.gin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewritten gin arg: --gin_bindings=MODEL_DIR = 'out/HoC/base_HoC'\n",
      "Rewritten gin arg: --gin_bindings=INITIAL_CHECKPOINT_PATH = 'biot5x_base'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0129 04:01:49.038258 140222066542400 resource_reader.py:50] system_path_file_exists:configs/runs/pretrain.gin\n",
      "E0129 04:01:49.039628 140222066542400 resource_reader.py:55] Path not found: configs/runs/pretrain.gin\n",
      "I0129 04:01:49.040354 140222066542400 resource_reader.py:50] system_path_file_exists:./configs/runs/pretrain.gin\n",
      "E0129 04:01:49.040996 140222066542400 resource_reader.py:55] Path not found: ./configs/runs/pretrain.gin\n",
      "I0129 04:01:49.061709 140222066542400 gin_utils.py:86] Gin Configuration:\n",
      "I0129 04:01:49.066993 140222066542400 gin_utils.py:88] from __gin__ import dynamic_registration\n",
      "I0129 04:01:49.067856 140222066542400 gin_utils.py:88] import __main__ as train_script\n",
      "I0129 04:01:49.068316 140222066542400 gin_utils.py:88] import seqio\n",
      "I0129 04:01:49.068759 140222066542400 gin_utils.py:88] from t5.data import mixtures\n",
      "I0129 04:01:49.069275 140222066542400 gin_utils.py:88] from t5x import adafactor\n",
      "I0129 04:01:49.069805 140222066542400 gin_utils.py:88] from t5x.examples.t5 import network\n",
      "I0129 04:01:49.070174 140222066542400 gin_utils.py:88] from t5x import gin_utils\n",
      "I0129 04:01:49.070509 140222066542400 gin_utils.py:88] from t5x import models\n",
      "I0129 04:01:49.070953 140222066542400 gin_utils.py:88] from t5x import partitioning\n",
      "I0129 04:01:49.071291 140222066542400 gin_utils.py:88] from t5x import trainer\n",
      "I0129 04:01:49.071626 140222066542400 gin_utils.py:88] from t5x import utils\n",
      "I0129 04:01:49.071971 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.072334 140222066542400 gin_utils.py:88] # Macros:\n",
      "I0129 04:01:49.072671 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.073054 140222066542400 gin_utils.py:88] BATCH_SIZE = 128\n",
      "I0129 04:01:49.073392 140222066542400 gin_utils.py:88] DROPOUT_RATE = 0.1\n",
      "I0129 04:01:49.073755 140222066542400 gin_utils.py:88] EVAL_PERIOD = 4900\n",
      "I0129 04:01:49.074107 140222066542400 gin_utils.py:88] INITIAL_CHECKPOINT_PATH = 'biot5x_base'\n",
      "I0129 04:01:49.074454 140222066542400 gin_utils.py:88] LABEL_SMOOTHING = 0.0\n",
      "I0129 04:01:49.074807 140222066542400 gin_utils.py:88] LEARNING_RATE = 0.001\n",
      "I0129 04:01:49.075173 140222066542400 gin_utils.py:88] LOSS_NORMALIZING_FACTOR = None\n",
      "I0129 04:01:49.075508 140222066542400 gin_utils.py:88] MIXTURE_OR_TASK_MODULE = None\n",
      "I0129 04:01:49.075862 140222066542400 gin_utils.py:88] MIXTURE_OR_TASK_NAME = 'HoC'\n",
      "I0129 04:01:49.076266 140222066542400 gin_utils.py:88] MODEL = @models.EncoderDecoderModel()\n",
      "I0129 04:01:49.076589 140222066542400 gin_utils.py:88] MODEL_DIR = 'out/HoC/base_HoC'\n",
      "I0129 04:01:49.076946 140222066542400 gin_utils.py:88] OPTIMIZER = @adafactor.Adafactor()\n",
      "I0129 04:01:49.077265 140222066542400 gin_utils.py:88] RANDOM_SEED = 0\n",
      "I0129 04:01:49.077584 140222066542400 gin_utils.py:88] SAVE_PERIOD = 4900\n",
      "I0129 04:01:49.077959 140222066542400 gin_utils.py:88] SHUFFLE_TRAIN_EXAMPLES = True\n",
      "I0129 04:01:49.078294 140222066542400 gin_utils.py:88] TASK_FEATURE_LENGTHS = {'inputs': 512, 'targets': 20}\n",
      "I0129 04:01:49.078644 140222066542400 gin_utils.py:88] TRAIN_STEPS = 1204900\n",
      "I0129 04:01:49.078985 140222066542400 gin_utils.py:88] USE_CACHED_TASKS = True\n",
      "I0129 04:01:49.079322 140222066542400 gin_utils.py:88] USE_HARDWARE_RNG = False\n",
      "I0129 04:01:49.079688 140222066542400 gin_utils.py:88] VOCABULARY = @seqio.SentencePieceVocabulary()\n",
      "I0129 04:01:49.080014 140222066542400 gin_utils.py:88] WARMUP_STEPS = 10\n",
      "I0129 04:01:49.080366 140222066542400 gin_utils.py:88] Z_LOSS = 0.0001\n",
      "I0129 04:01:49.080754 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.081072 140222066542400 gin_utils.py:88] # Parameters for adafactor.Adafactor:\n",
      "I0129 04:01:49.081455 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.081794 140222066542400 gin_utils.py:88] adafactor.Adafactor.decay_rate = 0.8\n",
      "I0129 04:01:49.082138 140222066542400 gin_utils.py:88] adafactor.Adafactor.logical_factor_rules = \\\n",
      "I0129 04:01:49.082494 140222066542400 gin_utils.py:88]     @adafactor.standard_logical_factor_rules()\n",
      "I0129 04:01:49.082866 140222066542400 gin_utils.py:88] adafactor.Adafactor.step_offset = 0\n",
      "I0129 04:01:49.083205 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.083606 140222066542400 gin_utils.py:88] # Parameters for utils.CheckpointConfig:\n",
      "I0129 04:01:49.083996 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.084340 140222066542400 gin_utils.py:88] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n",
      "I0129 04:01:49.084702 140222066542400 gin_utils.py:88] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n",
      "I0129 04:01:49.085049 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.085415 140222066542400 gin_utils.py:88] # Parameters for utils.create_learning_rate_scheduler:\n",
      "I0129 04:01:49.085784 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.088334 140222066542400 gin_utils.py:88] utils.create_learning_rate_scheduler.base_learning_rate = %LEARNING_RATE\n",
      "I0129 04:01:49.088947 140222066542400 gin_utils.py:88] utils.create_learning_rate_scheduler.factors = 'constant'\n",
      "I0129 04:01:49.089496 140222066542400 gin_utils.py:88] utils.create_learning_rate_scheduler.warmup_steps = %WARMUP_STEPS\n",
      "I0129 04:01:49.089890 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.090337 140222066542400 gin_utils.py:88] # Parameters for infer_eval/utils.DatasetConfig:\n",
      "I0129 04:01:49.090762 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.091187 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n",
      "I0129 04:01:49.091555 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n",
      "I0129 04:01:49.091939 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.seed = 0\n",
      "I0129 04:01:49.092372 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.shuffle = False\n",
      "I0129 04:01:49.092809 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.split = 'predict'\n",
      "I0129 04:01:49.093184 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.task_feature_lengths = None\n",
      "I0129 04:01:49.093673 140222066542400 gin_utils.py:88] infer_eval/utils.DatasetConfig.use_cached = False\n",
      "I0129 04:01:49.094220 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.094863 140222066542400 gin_utils.py:88] # Parameters for train/utils.DatasetConfig:\n",
      "I0129 04:01:49.095306 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.095830 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n",
      "I0129 04:01:49.096308 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n",
      "I0129 04:01:49.096887 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n",
      "I0129 04:01:49.097485 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.pack = True\n",
      "I0129 04:01:49.098128 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.seed = 0\n",
      "I0129 04:01:49.098623 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n",
      "I0129 04:01:49.099115 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.split = 'train'\n",
      "I0129 04:01:49.099541 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n",
      "I0129 04:01:49.100067 140222066542400 gin_utils.py:88] train/utils.DatasetConfig.use_cached = False\n",
      "I0129 04:01:49.100611 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.101112 140222066542400 gin_utils.py:88] # Parameters for train_eval/utils.DatasetConfig:\n",
      "I0129 04:01:49.101716 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.104542 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.batch_size = %BATCH_SIZE\n",
      "I0129 04:01:49.104973 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n",
      "I0129 04:01:49.105404 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n",
      "I0129 04:01:49.105795 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.pack = True\n",
      "I0129 04:01:49.106231 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.seed = 0\n",
      "I0129 04:01:49.106663 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.shuffle = False\n",
      "I0129 04:01:49.107076 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.split = 'validation'\n",
      "I0129 04:01:49.107655 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n",
      "I0129 04:01:49.108123 140222066542400 gin_utils.py:88] train_eval/utils.DatasetConfig.use_cached = False\n",
      "I0129 04:01:49.108625 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.109101 140222066542400 gin_utils.py:88] # Parameters for models.EncoderDecoderModel:\n",
      "I0129 04:01:49.109568 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.110037 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.input_vocabulary = %VOCABULARY\n",
      "I0129 04:01:49.110543 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.label_smoothing = %LABEL_SMOOTHING\n",
      "I0129 04:01:49.111080 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n",
      "I0129 04:01:49.111554 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.module = @network.Transformer()\n",
      "I0129 04:01:49.112049 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.optimizer_def = %OPTIMIZER\n",
      "I0129 04:01:49.112497 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.output_vocabulary = %VOCABULARY\n",
      "I0129 04:01:49.112975 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.z_loss = %Z_LOSS\n",
      "I0129 04:01:49.113415 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.113846 140222066542400 gin_utils.py:88] # Parameters for models.EncoderDecoderModel.predict_batch_with_aux:\n",
      "I0129 04:01:49.114305 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.114708 140222066542400 gin_utils.py:88] models.EncoderDecoderModel.predict_batch_with_aux.num_decodes = 4\n",
      "I0129 04:01:49.115117 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.115543 140222066542400 gin_utils.py:88] # Parameters for seqio.Evaluator:\n",
      "I0129 04:01:49.115929 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.116374 140222066542400 gin_utils.py:88] seqio.Evaluator.logger_cls = \\\n",
      "I0129 04:01:49.116811 140222066542400 gin_utils.py:88]     [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]\n",
      "I0129 04:01:49.117204 140222066542400 gin_utils.py:88] seqio.Evaluator.num_examples = None\n",
      "I0129 04:01:49.117602 140222066542400 gin_utils.py:88] seqio.Evaluator.use_memory_cache = True\n",
      "I0129 04:01:49.118061 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.118400 140222066542400 gin_utils.py:88] # Parameters for partitioning.PjitPartitioner:\n",
      "I0129 04:01:49.118781 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.119182 140222066542400 gin_utils.py:88] partitioning.PjitPartitioner.logical_axis_rules = \\\n",
      "I0129 04:01:49.119618 140222066542400 gin_utils.py:88]     @partitioning.standard_logical_axis_rules()\n",
      "I0129 04:01:49.120002 140222066542400 gin_utils.py:88] partitioning.PjitPartitioner.model_parallel_submesh = (1, 1)\n",
      "I0129 04:01:49.120433 140222066542400 gin_utils.py:88] partitioning.PjitPartitioner.num_partitions = 1\n",
      "I0129 04:01:49.120847 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.121239 140222066542400 gin_utils.py:88] # Parameters for utils.RestoreCheckpointConfig:\n",
      "I0129 04:01:49.121623 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.122051 140222066542400 gin_utils.py:88] utils.RestoreCheckpointConfig.dtype = 'float32'\n",
      "I0129 04:01:49.122449 140222066542400 gin_utils.py:88] utils.RestoreCheckpointConfig.mode = 'specific'\n",
      "I0129 04:01:49.122817 140222066542400 gin_utils.py:88] utils.RestoreCheckpointConfig.path = %INITIAL_CHECKPOINT_PATH\n",
      "I0129 04:01:49.123263 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.123740 140222066542400 gin_utils.py:88] # Parameters for utils.SaveCheckpointConfig:\n",
      "I0129 04:01:49.124147 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.124632 140222066542400 gin_utils.py:88] utils.SaveCheckpointConfig.dtype = 'float32'\n",
      "I0129 04:01:49.125037 140222066542400 gin_utils.py:88] utils.SaveCheckpointConfig.keep = None\n",
      "I0129 04:01:49.125445 140222066542400 gin_utils.py:88] utils.SaveCheckpointConfig.period = %SAVE_PERIOD\n",
      "I0129 04:01:49.125875 140222066542400 gin_utils.py:88] utils.SaveCheckpointConfig.save_dataset = False\n",
      "I0129 04:01:49.126246 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.126626 140222066542400 gin_utils.py:88] # Parameters for seqio.SentencePieceVocabulary:\n",
      "I0129 04:01:49.127054 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.127455 140222066542400 gin_utils.py:88] seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n",
      "I0129 04:01:49.127927 140222066542400 gin_utils.py:88]     'gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model'\n",
      "I0129 04:01:49.128398 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.128797 140222066542400 gin_utils.py:88] # Parameters for network.T5Config:\n",
      "I0129 04:01:49.129163 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.129574 140222066542400 gin_utils.py:88] network.T5Config.dropout_rate = %DROPOUT_RATE\n",
      "I0129 04:01:49.129972 140222066542400 gin_utils.py:88] network.T5Config.dtype = 'bfloat16'\n",
      "I0129 04:01:49.130334 140222066542400 gin_utils.py:88] network.T5Config.emb_dim = 768\n",
      "I0129 04:01:49.130711 140222066542400 gin_utils.py:88] network.T5Config.head_dim = 64\n",
      "I0129 04:01:49.131147 140222066542400 gin_utils.py:88] network.T5Config.logits_via_embedding = True\n",
      "I0129 04:01:49.131529 140222066542400 gin_utils.py:88] network.T5Config.mlp_activations = ('relu',)\n",
      "I0129 04:01:49.131973 140222066542400 gin_utils.py:88] network.T5Config.mlp_dim = 3072\n",
      "I0129 04:01:49.132393 140222066542400 gin_utils.py:88] network.T5Config.num_decoder_layers = 12\n",
      "I0129 04:01:49.132758 140222066542400 gin_utils.py:88] network.T5Config.num_encoder_layers = 12\n",
      "I0129 04:01:49.133147 140222066542400 gin_utils.py:88] network.T5Config.num_heads = 12\n",
      "I0129 04:01:49.133584 140222066542400 gin_utils.py:88] network.T5Config.vocab_size = 32128\n",
      "I0129 04:01:49.133990 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.134382 140222066542400 gin_utils.py:88] # Parameters for train_script.train:\n",
      "I0129 04:01:49.134785 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.135185 140222066542400 gin_utils.py:88] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n",
      "I0129 04:01:49.135599 140222066542400 gin_utils.py:88] train_script.train.eval_period = %EVAL_PERIOD\n",
      "I0129 04:01:49.135999 140222066542400 gin_utils.py:88] train_script.train.eval_steps = 20\n",
      "I0129 04:01:49.136391 140222066542400 gin_utils.py:88] train_script.train.infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()\n",
      "I0129 04:01:49.136780 140222066542400 gin_utils.py:88] train_script.train.inference_evaluator_cls = @seqio.Evaluator\n",
      "I0129 04:01:49.137173 140222066542400 gin_utils.py:88] train_script.train.model = %MODEL\n",
      "I0129 04:01:49.137572 140222066542400 gin_utils.py:88] train_script.train.model_dir = %MODEL_DIR\n",
      "I0129 04:01:49.137928 140222066542400 gin_utils.py:88] train_script.train.partitioner = @partitioning.PjitPartitioner()\n",
      "I0129 04:01:49.138355 140222066542400 gin_utils.py:88] train_script.train.random_seed = 0\n",
      "I0129 04:01:49.138716 140222066542400 gin_utils.py:88] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n",
      "I0129 04:01:49.139093 140222066542400 gin_utils.py:88] train_script.train.total_steps = %TRAIN_STEPS\n",
      "I0129 04:01:49.139427 140222066542400 gin_utils.py:88] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n",
      "I0129 04:01:49.139835 140222066542400 gin_utils.py:88] train_script.train.train_eval_dataset_cfg = @train_eval/utils.DatasetConfig()\n",
      "I0129 04:01:49.140171 140222066542400 gin_utils.py:88] train_script.train.trainer_cls = @trainer.Trainer\n",
      "I0129 04:01:49.140504 140222066542400 gin_utils.py:88] train_script.train.use_hardware_rng = True\n",
      "I0129 04:01:49.140860 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.141212 140222066542400 gin_utils.py:88] # Parameters for trainer.Trainer:\n",
      "I0129 04:01:49.141587 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.141974 140222066542400 gin_utils.py:88] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n",
      "I0129 04:01:49.142314 140222066542400 gin_utils.py:88] trainer.Trainer.num_microbatches = None\n",
      "I0129 04:01:49.142674 140222066542400 gin_utils.py:88] \n",
      "I0129 04:01:49.143027 140222066542400 gin_utils.py:88] # Parameters for network.Transformer:\n",
      "I0129 04:01:49.143426 140222066542400 gin_utils.py:88] # ==============================================================================\n",
      "I0129 04:01:49.143796 140222066542400 gin_utils.py:88] network.Transformer.config = @network.T5Config()\n",
      "I0129 04:01:49.145196 140222066542400 partitioning.py:498] `activation_partitioning_dims` = 1, `parameter_partitioning_dims` = 1\n",
      "E0129 04:01:49.145677 140222066542400 partitioning.py:621] `model_parallel_submesh` must be either None or a 4-tuple. Got Got `num_partitions=(1, 1)`. A ValueError will be raised beginning March 1, 2022.\n",
      "E0129 04:01:49.146046 140222066542400 partitioning.py:627] At most one of `num_partitions` or `model_parallel_submesh` can be set. Got `num_partitions=1` and `model_parallel_submesh`=(1, 1). A ValueError will be raised beginning March 21, 2022.\n",
      "I0129 04:01:49.151097 140222066542400 xla_bridge.py:355] Unable to initialize backend 'tpu_driver': NOT_FOUND: Unable to find driver in registry given worker: \n",
      "I0129 04:01:49.318848 140222066542400 xla_bridge.py:355] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: \"rocm\". Available platform names are: Interpreter CUDA Host\n",
      "I0129 04:01:49.319899 140222066542400 xla_bridge.py:355] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "I0129 04:01:49.320518 140222066542400 xla_bridge.py:355] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.\n",
      "I0129 04:01:49.321003 140222066542400 finetune_biot5x.py:200] Process ID: 0\n",
      "I0129 04:01:49.325726 140222066542400 finetune_biot5x.py:204] GlobalDeviceArray enabled.\n",
      "I0129 04:01:49.326089 140222066542400 finetune_biot5x.py:244] Using fast RngBitGenerator PRNG for initialization and dropout.\n",
      "W0129 04:01:49.326411 140222066542400 finetune_biot5x.py:251] When using hardware RNG with a fixed seed, repeatability is only guaranteed for fixed hardware and partitioning schemes and for a fixed version of this code and its dependencies.\n",
      "/usr/local/lib/python3.9/dist-packages/jax/_src/lib/xla_bridge.py:555: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
      "  warnings.warn(\n",
      "I0129 04:01:51.276332 140222066542400 partitioning.py:196] last device coords : (0, 0)\n",
      "last local device coords: (0, 0)\n",
      "W0129 04:01:51.277070 140222066542400 partitioning.py:242] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.\n",
      "I0129 04:01:51.277716 140222066542400 partitioning.py:300] global_mesh axis_names: ('data', 'model')\n",
      "I0129 04:01:51.278216 140222066542400 partitioning.py:301] global_mesh devices: [[StreamExecutorGpuDevice(id=0, process_index=0, slice_index=0)]]\n",
      "I0129 04:01:51.279629 140222066542400 partitioning.py:302] global_mesh devices shape: (1, 1)\n",
      "2023-01-29 04:01:51.298185: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "I0129 04:01:52.493552 140222066542400 utils.py:1623] Initializing dataset for task 'HoC' with a replica batch size of 128 and a seed of 0\n",
      "I0129 04:01:52.494468 140222066542400 dataset_providers.py:1329] Sharding at the data source: 0 of 1\n",
      "W0129 04:01:52.494969 140222066542400 dataset_providers.py:391] Using an uncached FunctionDataset for training is not recommended since it often results in insufficient shuffling on restarts, resulting in overfitting. It is highly recommended that you cache this task before training with it or use a data source that supports lower-level shuffling (e.g., FileDataSource).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0129 04:01:52.540020 140222066542400 deprecation.py:350] From /usr/local/lib/python3.9/dist-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "W0129 04:01:53.393727 140222066542400 utils.py:742] Features not in `features_length` will be removed during packing: {'inputs_pretokenized', 'targets_pretokenized'}\n",
      "I0129 04:01:54.499537 140222066542400 dataset_providers.py:2066] The output dataset from seqio.get_dataset has the following features\n",
      "I0129 04:01:54.500272 140222066542400 dataset_providers.py:2072] feature: encoder_input_tokens \t shape: [128, 512] \t dtype: int32\n",
      "I0129 04:01:54.500725 140222066542400 dataset_providers.py:2072] feature: decoder_target_tokens \t shape: [128, 20] \t dtype: int32\n",
      "I0129 04:01:54.501192 140222066542400 dataset_providers.py:2072] feature: decoder_input_tokens \t shape: [128, 20] \t dtype: int32\n",
      "I0129 04:01:54.501556 140222066542400 dataset_providers.py:2072] feature: decoder_loss_weights \t shape: [128, 20] \t dtype: int32\n",
      "I0129 04:01:54.501913 140222066542400 dataset_providers.py:2072] feature: encoder_segment_ids \t shape: [128, 512] \t dtype: int32\n",
      "I0129 04:01:54.502259 140222066542400 dataset_providers.py:2072] feature: decoder_segment_ids \t shape: [128, 20] \t dtype: int32\n",
      "I0129 04:01:54.502599 140222066542400 dataset_providers.py:2072] feature: encoder_positions \t shape: [128, 512] \t dtype: int32\n",
      "I0129 04:01:54.502941 140222066542400 dataset_providers.py:2072] feature: decoder_positions \t shape: [128, 20] \t dtype: int32\n",
      "I0129 04:01:54.822661 140222066542400 utils.py:1709] Task HoC has no 'validation' split; skipping training evaluation.\n",
      "W0129 04:01:54.823656 140222066542400 finetune_biot5x.py:305] No train_eval datasets loaded from config `train_eval_dataset_cfg`: DatasetConfig(mixture_or_task_name='HoC', task_feature_lengths={'inputs': 512, 'targets': 20}, split='validation', batch_size=128, shuffle=False, seed=0, use_cached=False, pack=True, use_custom_packing_ops=False, module=None, use_memory_cache=True, trim_output_features=True)\n",
      "W0129 04:01:57.663394 140222066542400 adafactor.py:346] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.665127 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.666874 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.668578 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.670174 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.671672 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.673196 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.675154 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.676384 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.677623 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.678878 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.680427 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.681957 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.683533 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.684953 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.686368 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.687784 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.689270 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.690825 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.692237 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.693964 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.695254 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.696542 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.697792 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.699200 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.700708 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.702187 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.703621 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.705017 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.706547 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.707932 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.709426 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.710983 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.712493 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.714156 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.715363 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.716712 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.718434 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.719823 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.721284 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.722896 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.724340 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.725841 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.728216 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.730052 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.731466 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.733066 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.734421 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.735687 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.736945 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.738391 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.739849 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.741227 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.742657 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.744038 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.746173 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.747487 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.748958 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.750391 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.751792 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.752992 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.754347 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.755531 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.757004 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.758403 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.759827 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.761173 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.762536 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.763930 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.765298 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.766712 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.768120 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.769521 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.770648 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.771816 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.773061 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.774466 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.775782 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.777180 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.778618 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.780755 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.782441 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.783973 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.785498 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.786863 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.788247 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.789909 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.791111 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.792261 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.793665 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.795153 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.796528 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.797978 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.799423 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.800801 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.802209 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.803551 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.804941 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.806421 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.807591 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.808753 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.809930 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.811269 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.812626 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.815628 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.817160 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.818602 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.820071 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.821548 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.823288 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.824893 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.826524 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.827817 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.829112 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.830408 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.831888 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.844942 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.846743 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.848445 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.850127 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.851716 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.853392 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.855068 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.856511 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.857952 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.859240 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.860483 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.861729 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.863248 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.865329 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.867045 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.868638 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.870107 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.871719 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.873718 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.875101 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.876566 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.878010 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.879259 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.880529 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.881849 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.884570 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.886542 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.888991 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.891047 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/encoder_decoder_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.894575 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/encoder_decoder_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.896753 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/encoder_decoder_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.899212 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/encoder_decoder_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.901492 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.903434 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.904934 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/pre_cross_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.906339 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.907588 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.908928 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.910575 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.912239 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.913991 140222066542400 adafactor.py:346] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.915489 140222066542400 adafactor.py:346] Since rank of parameter decoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "W0129 04:01:57.916832 140222066542400 adafactor.py:346] Since rank of parameter encoder/encoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.918180 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.919616 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.921031 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.922506 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.924071 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.925791 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.927382 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.928610 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.929929 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.931364 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.932734 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.934375 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.935737 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.937085 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.939129 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.940553 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.941912 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.943526 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.945122 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.946644 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.948136 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.949637 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.951284 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.952681 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.954069 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.955648 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.957259 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.958866 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.960242 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.961843 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.963373 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.964836 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.966032 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.967656 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.969049 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.970648 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.972380 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.974033 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.975814 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.977355 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.978815 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.980644 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.983239 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.984920 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.986379 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.987818 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.989283 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.990681 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:57.992123 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.993705 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:57.995176 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.996694 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.998195 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:57.999560 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.001158 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.002738 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.003999 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.005306 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.006850 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.008904 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.010566 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.012119 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.013651 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.015323 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.016506 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.018363 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.019706 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.021121 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.022559 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.024019 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.025640 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.026904 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.028491 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.030083 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.031529 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.032943 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.034516 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.036418 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.038117 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.039349 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.040568 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.042031 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.043526 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.044992 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.046437 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.047975 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.050434 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.051660 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.053477 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.054916 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.056256 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.057683 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.059062 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/mlp/wi/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.\n",
      "W0129 04:01:58.060787 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "W0129 04:01:58.062228 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/pre_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.063396 140222066542400 adafactor.py:346] Since rank of parameter encoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.\n",
      "W0129 04:01:58.064656 140222066542400 adafactor.py:346] Since rank of parameter encoder/relpos_bias/rel_embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.\n",
      "W0129 04:01:58.065922 140222066542400 adafactor.py:346] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.\n",
      "I0129 04:01:58.533227 140222066542400 utils.py:927] No checkpoints found in specified directory: out/HoC/base_HoC\n",
      "I0129 04:01:58.534030 140222066542400 checkpoints.py:590] Checkpointing using GDA format is enabled.\n",
      "I0129 04:01:58.632024 140222066542400 checkpoints.py:590] Checkpointing using GDA format is enabled.\n",
      "I0129 04:01:58.728862 140222066542400 utils.py:483] Initializing parameters from specific T5X checkpoint biot5x_base\n",
      "I0129 04:01:58.730696 140222066542400 checkpoints.py:1032] Restoring from checkpoint: biot5x_base/checkpoint\n",
      "I0129 04:01:58.759277 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/m\n",
      "I0129 04:01:58.759921 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v\n",
      "I0129 04:01:58.760307 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_col\n",
      "I0129 04:01:58.761095 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/decoder_norm/scale/v_row\n",
      "I0129 04:01:58.761434 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.761830 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.762184 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.762563 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.762928 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.763319 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.763716 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.764125 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.764487 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.764861 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.765237 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.765610 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.765953 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.766327 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.766723 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.767078 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.767431 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi/kernel/m\n",
      "I0129 04:01:58.767777 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi/kernel/v\n",
      "I0129 04:01:58.768150 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.768503 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.768865 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/m\n",
      "I0129 04:01:58.769232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v\n",
      "I0129 04:01:58.769627 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.770004 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.770355 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.770731 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.771079 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.771436 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.771822 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.772181 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.772563 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.772954 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.773538 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.773846 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.774217 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.774640 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.775019 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/m\n",
      "I0129 04:01:58.775460 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v\n",
      "I0129 04:01:58.775830 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.776179 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.776538 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/m\n",
      "I0129 04:01:58.776911 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v\n",
      "I0129 04:01:58.777287 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.777690 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.778234 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/m\n",
      "I0129 04:01:58.778585 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v\n",
      "I0129 04:01:58.778895 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.779264 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.779628 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/m\n",
      "I0129 04:01:58.780006 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v\n",
      "I0129 04:01:58.780370 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.780723 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_0/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.781117 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.781512 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.781927 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.782307 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.782664 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.783016 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.783434 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.783817 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.784203 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.784705 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.785034 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.785402 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.785786 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.786134 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.786505 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.786874 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.787243 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi/kernel/m\n",
      "I0129 04:01:58.787910 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi/kernel/v\n",
      "I0129 04:01:58.788270 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.788619 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.788986 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/m\n",
      "I0129 04:01:58.789305 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v\n",
      "I0129 04:01:58.789672 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.790081 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.790399 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.790755 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.791139 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.791525 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.791947 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.792358 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.792708 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.793058 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.793468 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.793816 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.794215 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.794584 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.794995 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/m\n",
      "I0129 04:01:58.795341 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v\n",
      "I0129 04:01:58.795709 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.796077 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.796442 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/m\n",
      "I0129 04:01:58.796804 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v\n",
      "I0129 04:01:58.797423 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.797778 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.798149 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/m\n",
      "I0129 04:01:58.798494 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v\n",
      "I0129 04:01:58.798871 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.799233 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.799580 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/m\n",
      "I0129 04:01:58.799941 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v\n",
      "I0129 04:01:58.800316 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.800675 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_1/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.801192 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.801574 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.801970 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.802330 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.802713 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.803081 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.803424 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.803782 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.804150 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.804514 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.804908 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.805307 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.805686 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.806029 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.806393 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.806927 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.807233 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi/kernel/m\n",
      "I0129 04:01:58.807626 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi/kernel/v\n",
      "I0129 04:01:58.808038 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.808407 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.808786 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/m\n",
      "I0129 04:01:58.809146 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v\n",
      "I0129 04:01:58.809546 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.809905 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.810275 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.810638 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.811003 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.811393 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.811756 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.812118 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.812474 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.812859 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.813242 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.813617 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.813974 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.814327 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.814702 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/m\n",
      "I0129 04:01:58.815057 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v\n",
      "I0129 04:01:58.815406 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.815799 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.816169 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/m\n",
      "I0129 04:01:58.816565 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v\n",
      "I0129 04:01:58.816907 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.817294 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.817646 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/m\n",
      "I0129 04:01:58.818007 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v\n",
      "I0129 04:01:58.818377 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.818739 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.819141 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/m\n",
      "I0129 04:01:58.819500 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v\n",
      "I0129 04:01:58.819843 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.820233 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_10/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.820606 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.821188 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.821539 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.821921 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.822277 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.822639 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.823009 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.823394 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.823784 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.824155 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.824528 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.824885 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.825223 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.825594 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.826126 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.826459 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.826823 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi/kernel/m\n",
      "I0129 04:01:58.827212 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi/kernel/v\n",
      "I0129 04:01:58.827585 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.830025 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.830436 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/m\n",
      "I0129 04:01:58.830808 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v\n",
      "I0129 04:01:58.831175 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.831540 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.831935 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.832302 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.832666 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.833049 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.833476 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.833867 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.834216 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.834592 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.834972 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.835333 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.835683 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.836080 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.836441 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/m\n",
      "I0129 04:01:58.836809 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v\n",
      "I0129 04:01:58.837211 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.837618 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.838020 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/m\n",
      "I0129 04:01:58.838372 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v\n",
      "I0129 04:01:58.838753 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.839127 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.839494 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/m\n",
      "I0129 04:01:58.839880 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v\n",
      "I0129 04:01:58.840255 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.840649 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.840984 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/m\n",
      "I0129 04:01:58.841424 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v\n",
      "I0129 04:01:58.841843 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.842216 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_11/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.842606 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.843001 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.843414 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.843766 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.844141 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.844509 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.844882 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.845330 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.845690 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.846055 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.846427 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.846851 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.847293 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.847961 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.848271 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.848658 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.849048 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi/kernel/m\n",
      "I0129 04:01:58.849461 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi/kernel/v\n",
      "I0129 04:01:58.849881 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.850245 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.850617 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/m\n",
      "I0129 04:01:58.851046 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v\n",
      "I0129 04:01:58.851415 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.851797 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.852147 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.852515 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.852884 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.853296 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.853685 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.854034 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.854437 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.854805 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.855152 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.855538 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.855926 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.856318 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.856715 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/m\n",
      "I0129 04:01:58.857092 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v\n",
      "I0129 04:01:58.857466 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.857836 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.858232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/m\n",
      "I0129 04:01:58.858629 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v\n",
      "I0129 04:01:58.859011 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.859382 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.859727 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/m\n",
      "I0129 04:01:58.860087 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v\n",
      "I0129 04:01:58.860452 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.860840 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.861229 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/m\n",
      "I0129 04:01:58.861578 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v\n",
      "I0129 04:01:58.861959 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.862340 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_2/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.862686 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.863078 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.863426 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.863817 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.864187 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.864547 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.864898 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.865324 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.865678 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.866030 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.866371 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.866744 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.867108 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.867468 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.867829 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.868229 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.868584 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi/kernel/m\n",
      "I0129 04:01:58.869232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi/kernel/v\n",
      "I0129 04:01:58.869573 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.869955 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.870343 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/m\n",
      "I0129 04:01:58.870730 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v\n",
      "I0129 04:01:58.871114 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.871478 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.871890 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.872262 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.872616 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.872965 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.873373 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.873721 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.874109 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.874475 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.874852 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.875232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.875654 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.875996 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.876376 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/m\n",
      "I0129 04:01:58.876785 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v\n",
      "I0129 04:01:58.877156 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.877545 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.877898 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/m\n",
      "I0129 04:01:58.878286 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v\n",
      "I0129 04:01:58.878637 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.879019 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.879417 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/m\n",
      "I0129 04:01:58.879937 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v\n",
      "I0129 04:01:58.880513 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.881160 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.881599 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/m\n",
      "I0129 04:01:58.882033 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v\n",
      "I0129 04:01:58.882484 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.882840 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_3/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.883209 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.883608 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.883960 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.884359 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.884684 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.885053 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.885445 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.885781 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.886091 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.886431 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.886822 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.887156 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.887478 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.887805 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.888149 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.888485 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.888799 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi/kernel/m\n",
      "I0129 04:01:58.889171 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi/kernel/v\n",
      "I0129 04:01:58.889494 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.889824 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.890166 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/m\n",
      "I0129 04:01:58.890537 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v\n",
      "I0129 04:01:58.890877 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.891268 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.891682 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.892025 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.892362 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.892749 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.893072 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.893470 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.893783 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.894140 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.894471 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.894798 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.895128 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.895447 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.895771 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/m\n",
      "I0129 04:01:58.896094 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v\n",
      "I0129 04:01:58.896420 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.896753 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.897118 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/m\n",
      "I0129 04:01:58.897442 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v\n",
      "I0129 04:01:58.897771 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.898115 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.898511 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/m\n",
      "I0129 04:01:58.898831 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v\n",
      "I0129 04:01:58.899160 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.899495 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.899825 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/m\n",
      "I0129 04:01:58.900181 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v\n",
      "I0129 04:01:58.900551 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.900860 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_4/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.901204 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.901556 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.901914 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.902240 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.902648 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.902991 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.903359 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.903692 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.904022 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.904369 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.904699 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.905028 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.905387 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.905738 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.906055 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.906381 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.906752 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi/kernel/m\n",
      "I0129 04:01:58.907378 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi/kernel/v\n",
      "I0129 04:01:58.907669 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.908023 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.908365 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/m\n",
      "I0129 04:01:58.908709 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v\n",
      "I0129 04:01:58.909059 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.909432 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.909788 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.910115 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.910465 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.910825 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.911191 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.911524 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.911846 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.912178 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.912536 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.912866 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.913232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.913625 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.913985 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/m\n",
      "I0129 04:01:58.914328 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v\n",
      "I0129 04:01:58.914680 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.915043 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.915376 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/m\n",
      "I0129 04:01:58.915699 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v\n",
      "I0129 04:01:58.916064 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.916400 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.916731 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/m\n",
      "I0129 04:01:58.917090 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v\n",
      "I0129 04:01:58.917446 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.917771 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.918097 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/m\n",
      "I0129 04:01:58.918442 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v\n",
      "I0129 04:01:58.918830 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.919227 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_5/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.919838 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.920136 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.920520 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.920867 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.921212 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.921720 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.922100 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.922440 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.922819 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.923170 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.923490 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.923833 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.924186 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.924548 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.924932 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.925354 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.925858 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi/kernel/m\n",
      "I0129 04:01:58.926248 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi/kernel/v\n",
      "I0129 04:01:58.926672 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.927087 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.927416 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/m\n",
      "I0129 04:01:58.932159 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v\n",
      "I0129 04:01:58.932568 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.932984 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.933373 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.933721 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.934079 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.934414 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.934785 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.935122 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.935501 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.935842 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.936197 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.936538 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.936869 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.937238 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.937712 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/m\n",
      "I0129 04:01:58.938165 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v\n",
      "I0129 04:01:58.938541 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.938885 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.939227 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/m\n",
      "I0129 04:01:58.939673 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v\n",
      "I0129 04:01:58.940049 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.940485 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.940879 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/m\n",
      "I0129 04:01:58.941274 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v\n",
      "I0129 04:01:58.941623 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.941992 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.942319 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/m\n",
      "I0129 04:01:58.942697 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v\n",
      "I0129 04:01:58.943045 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.943436 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_6/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.943742 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.944107 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.944465 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.944819 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.945216 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.946939 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.947258 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.947602 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.947939 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.948271 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.948611 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.948968 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.950799 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.951152 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.951533 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.951886 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.952226 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi/kernel/m\n",
      "I0129 04:01:58.952580 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi/kernel/v\n",
      "I0129 04:01:58.952933 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.953388 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.953824 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/m\n",
      "I0129 04:01:58.954178 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v\n",
      "I0129 04:01:58.954559 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.956338 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.956723 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.957056 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.957430 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.957776 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.958109 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.958449 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.958788 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.959120 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.959489 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.959836 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.960211 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.960489 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.960843 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/m\n",
      "I0129 04:01:58.961180 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v\n",
      "I0129 04:01:58.961543 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.961891 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.962281 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/m\n",
      "I0129 04:01:58.962632 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v\n",
      "I0129 04:01:58.962995 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.963387 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.963729 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/m\n",
      "I0129 04:01:58.964086 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v\n",
      "I0129 04:01:58.964350 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.964667 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.965014 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/m\n",
      "I0129 04:01:58.965419 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v\n",
      "I0129 04:01:58.965746 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.966085 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_7/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.966502 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.966846 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.967176 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.967535 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.967882 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.968206 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.968575 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.968915 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.969376 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.969666 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.969947 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.970234 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.970522 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.970900 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.971238 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.971626 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.971966 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi/kernel/m\n",
      "I0129 04:01:58.972315 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi/kernel/v\n",
      "I0129 04:01:58.972731 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.973067 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.973482 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/m\n",
      "I0129 04:01:58.973841 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v\n",
      "I0129 04:01:58.974146 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.974420 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.974790 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.975129 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.975487 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.975836 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.976194 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.976480 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.976750 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.977092 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.977448 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.977809 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.978169 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.978523 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.978797 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/m\n",
      "I0129 04:01:58.979115 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v\n",
      "I0129 04:01:58.979462 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.979799 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.980170 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/m\n",
      "I0129 04:01:58.980498 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v\n",
      "I0129 04:01:58.980837 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_col\n",
      "I0129 04:01:58.981370 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/out/kernel/v_row\n",
      "I0129 04:01:58.981885 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/m\n",
      "I0129 04:01:58.982261 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v\n",
      "I0129 04:01:58.982591 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_col\n",
      "I0129 04:01:58.982950 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/query/kernel/v_row\n",
      "I0129 04:01:58.983261 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/m\n",
      "I0129 04:01:58.983695 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v\n",
      "I0129 04:01:58.983992 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_col\n",
      "I0129 04:01:58.984336 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_8/self_attention/value/kernel/v_row\n",
      "I0129 04:01:58.984707 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/m\n",
      "I0129 04:01:58.985022 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v\n",
      "I0129 04:01:58.985389 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_col\n",
      "I0129 04:01:58.985701 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/key/kernel/v_row\n",
      "I0129 04:01:58.985996 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/m\n",
      "I0129 04:01:58.986395 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v\n",
      "I0129 04:01:58.986731 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_col\n",
      "I0129 04:01:58.987095 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/out/kernel/v_row\n",
      "I0129 04:01:58.987447 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/m\n",
      "I0129 04:01:58.987818 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v\n",
      "I0129 04:01:58.988214 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_col\n",
      "I0129 04:01:58.988537 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/query/kernel/v_row\n",
      "I0129 04:01:58.988841 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/m\n",
      "I0129 04:01:58.989232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v\n",
      "I0129 04:01:58.989586 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_col\n",
      "I0129 04:01:58.989938 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/encoder_decoder_attention/value/kernel/v_row\n",
      "I0129 04:01:58.990264 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi/kernel/m\n",
      "I0129 04:01:58.990628 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi/kernel/v\n",
      "I0129 04:01:58.990962 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi/kernel/v_col\n",
      "I0129 04:01:58.991364 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wi/kernel/v_row\n",
      "I0129 04:01:58.991715 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/m\n",
      "I0129 04:01:58.992127 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v\n",
      "I0129 04:01:58.992497 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_col\n",
      "I0129 04:01:58.992852 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/mlp/wo/kernel/v_row\n",
      "I0129 04:01:58.993240 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.993646 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.994006 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.994396 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_cross_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.994792 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m\n",
      "I0129 04:01:58.995433 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v\n",
      "I0129 04:01:58.995778 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col\n",
      "I0129 04:01:58.996101 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row\n",
      "I0129 04:01:58.996517 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m\n",
      "I0129 04:01:58.996825 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v\n",
      "I0129 04:01:58.997232 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col\n",
      "I0129 04:01:58.997612 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row\n",
      "I0129 04:01:58.998204 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/m\n",
      "I0129 04:01:58.998614 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v\n",
      "I0129 04:01:58.999132 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_col\n",
      "I0129 04:01:58.999510 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/key/kernel/v_row\n",
      "I0129 04:01:58.999837 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/m\n",
      "I0129 04:01:59.000152 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v\n",
      "I0129 04:01:59.000557 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_col\n",
      "I0129 04:01:59.000893 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/out/kernel/v_row\n",
      "I0129 04:01:59.001355 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/m\n",
      "I0129 04:01:59.001745 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v\n",
      "I0129 04:01:59.002241 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_col\n",
      "I0129 04:01:59.002562 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/query/kernel/v_row\n",
      "I0129 04:01:59.002961 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/m\n",
      "I0129 04:01:59.003288 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v\n",
      "I0129 04:01:59.003707 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_col\n",
      "I0129 04:01:59.004038 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/layers_9/self_attention/value/kernel/v_row\n",
      "I0129 04:01:59.004444 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/m\n",
      "I0129 04:01:59.004841 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v\n",
      "I0129 04:01:59.005255 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v_col\n",
      "I0129 04:01:59.005685 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/decoder/relpos_bias/rel_embedding/v_row\n",
      "I0129 04:01:59.006064 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/m\n",
      "I0129 04:01:59.006377 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v\n",
      "I0129 04:01:59.006698 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v_col\n",
      "I0129 04:01:59.007067 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/encoder_norm/scale/v_row\n",
      "I0129 04:01:59.007632 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/m\n",
      "I0129 04:01:59.007983 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v\n",
      "I0129 04:01:59.008411 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v_col\n",
      "I0129 04:01:59.008919 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/key/kernel/v_row\n",
      "I0129 04:01:59.009400 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/m\n",
      "I0129 04:01:59.009754 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v\n",
      "I0129 04:01:59.010095 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v_col\n",
      "I0129 04:01:59.010493 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/out/kernel/v_row\n",
      "I0129 04:01:59.010938 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/m\n",
      "I0129 04:01:59.011261 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v\n",
      "I0129 04:01:59.011657 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v_col\n",
      "I0129 04:01:59.011964 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/query/kernel/v_row\n",
      "I0129 04:01:59.012297 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/m\n",
      "I0129 04:01:59.012679 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v\n",
      "I0129 04:01:59.013047 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v_col\n",
      "I0129 04:01:59.013428 140222066542400 checkpoints.py:1085] Restoring key from ckpt: state/param_states/encoder/layers_0/attention/value/kernel/v_row\n"
     ]
    }
   ],
   "source": [
    "############################### NER: HoC ###############################\n",
    "model_size = 'base'\n",
    "task = 'HoC'\n",
    "train_file = f'biot5x/data/{task}/train_blurb.tsv'\n",
    "test_file = f'biot5x/data/{task}/test_blurb.tsv'\n",
    "dev_file = f'biot5x/data/{task}/dev_blurb.tsv'\n",
    "\n",
    "model_dir = f'out/{task}/{model_size}_{task}'\n",
    "pretrained_path=f'biot5x_{model_size}'\n",
    "gin_file = f'biot5x/configs/biot5x/finetune/base/{task}_blurb.gin'\n",
    "\n",
    "\n",
    "# micro F1 for HoC metric adapt from https://github.com/michiyasunaga/LinkBERT/blob/main/src/seqcls/utils_hoc.py\n",
    "metric = 'hoc'\n",
    "\n",
    "%run 'biot5x/src/finetune_biot5x.py' \\\n",
    "  --gin_file=\"{gin_file}\" \\\n",
    "  --gin.MODEL_DIR=\"'{model_dir}'\" \\\n",
    "  --gin.INITIAL_CHECKPOINT_PATH=\"'{pretrained_path}'\" \\\n",
    "  --task=\"{task}\" \\\n",
    "  --metric=\"{metric}\" \\\n",
    "  --train_file=\"{train_file}\" \\\n",
    "  --predict_file=\"{test_file}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e15634-d5b9-46c4-af87-32a0db36af0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
